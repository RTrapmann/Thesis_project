{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "from PIL import Image\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import datasets \n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art import attacks \n",
    "from art.attacks.evasion import DeepFool, CarliniL0Method, BasicIterativeMethod, CarliniL2Method\n",
    "from art.estimators.classification import PyTorchClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters  \n",
    "epochs = 3  #the nn will train 29 times \n",
    "learning_rate = 0.0001 #how much the weight will be updated each time \n",
    "batch_size = 64 \n",
    "classes = 43 \n",
    "img_size = 32\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(degrees = (0,1))  \n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.GTSRB(\n",
    "    root='./data', split = 'test', transform=transforms, download=True)\n",
    "\n",
    "# train loader \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size\n",
    ", shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<torch.utils.data.dataloader.DataLoader object at 0x7f72f7c42b80>, <torch.utils.data.dataloader.DataLoader object at 0x7f720bd06fd0>)\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True,\n",
    "                           num_workers=2):\n",
    "\n",
    "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "\n",
    "    # load the dataset\n",
    "\n",
    "    global base_dataset \n",
    "    base_dataset = torchvision.datasets.GTSRB(\n",
    "    root='./data', split = 'train', transform=transforms, download=True)\n",
    "\n",
    "    # TODO\n",
    "    split_datasets = torch.utils.data.random_split(base_dataset, [0.20,0.8])\n",
    "    global val_dataset, train_dataset\n",
    "    val_dataset = split_datasets[0]\n",
    "    train_dataset = split_datasets[1]\n",
    "    \n",
    "\n",
    "    global num_train \n",
    "    num_train= len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    global split \n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "    #train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    #train_sampler = SubsetRandomSampler(train_idx)\n",
    "    #valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    global train_loader \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        #sampler = train_sampler\n",
    "    )\n",
    "    global valid_loader \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        #sampler = valid_sampler\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "print(get_train_valid_loader(batch_size = 64, augment = True, random_seed = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "    \n",
    "    # temporarily change the style of the plots to seaborn \n",
    "    #plt.style.use('seaborn')\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='blue', label='Training loss') \n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    '''\n",
    "    Function for the training step of the training loop\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        y_hat= model(X) \n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function, without a learning step (backward pass)\n",
    "\n",
    "def validate(valid_loader, model, criterion, device):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "    \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Forward pass and record loss\n",
    "        y_hat= model(X)    # predicted\n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "        \n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=False)\n",
    "model.fc = torch.nn.Linear(2048,43)\n",
    "model.conv1 = torch.nn.Conv2d(3,64,kernel_size=5,stride=1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(random_seed)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:51:08 --- Epoch: 0\tTrain loss: 2.6637\tValid loss: 1.6539\tTrain accuracy: 54.07\tValid accuracy: 50.54\n",
      "15:51:40 --- Epoch: 1\tTrain loss: 1.0911\tValid loss: 0.6402\tTrain accuracy: 86.13\tValid accuracy: 80.24\n",
      "15:52:13 --- Epoch: 2\tTrain loss: 0.3680\tValid loss: 0.3263\tTrain accuracy: 95.62\tValid accuracy: 89.66\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet50.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weet je nog waarom we de layers hieronder hebben aangepast?\n",
    "# model = resnet50(pretrained=False)\n",
    "# model.fc = torch.nn.Linear(2048, 43)\n",
    "# model.conv1 = torch.nn.Conv2d(3, 64, kernel_size=5, stride=1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# We wrap our model in a PyTorchClassifier, which ART knows how to use\n",
    "classifier = PyTorchClassifier(\n",
    "    # model=WrappedModel(),\n",
    "    model=model,\n",
    "    clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
    "    loss=criterion,  # defined above\n",
    "    optimizer=optimizer,  # defined above\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=43,\n",
    "    device_type=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb878c336e30461aaef8fd583f83d793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cf5183dece4ef88da9b266fc9b9a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2861d097f74274af1df7f579572e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffbbb0c160d4c8b8225b8a4e44f7687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PGD - Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m true_label_numpy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([true_label])\n\u001b[1;32m     18\u001b[0m \u001b[39m# Genereer adversarial sample\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m X_adv \u001b[39m=\u001b[39m attack\u001b[39m.\u001b[39;49mgenerate(image_numpy)  \u001b[39m# shape == [1, 3, 32, 32]\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\u001b[39;00m\n\u001b[1;32m     21\u001b[0m normal_prediction \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(model(image))\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent.py:200\u001b[0m, in \u001b[0;36mProjectedGradientDescent.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39mGenerate adversarial samples and return them in an array.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[39m:return: An array holding the adversarial examples.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    199\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCreating adversarial samples.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 200\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attack\u001b[39m.\u001b[39;49mgenerate(x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py:154\u001b[0m, in \u001b[0;36mProjectedGradientDescentPyTorch.generate\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_eps()\n\u001b[1;32m    153\u001b[0m \u001b[39m# Set up targets\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_targets(x, y)\n\u001b[1;32m    156\u001b[0m \u001b[39m# Create dataset\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[39m# Here we need to make a distinction: if the masks are different for each input, we need to index\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[39m# those for the current batch. Otherwise (i.e. mask is meant to be broadcasted), keep it as it is.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_numpy.py:171\u001b[0m, in \u001b[0;36mProjectedGradientDescentCommon._set_targets\u001b[0;34m(self, x, y, classifier_mixin)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m# Use model predictions as correct outputs\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m classifier_mixin:\n\u001b[0;32m--> 171\u001b[0m     targets \u001b[39m=\u001b[39m get_labels_np_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator\u001b[39m.\u001b[39;49mpredict(x, batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size))\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m     targets \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mpredict(x, batch_size\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/estimators/classification/classifier.py:73\u001b[0m, in \u001b[0;36mInputFilter.__init__.<locals>.make_replacement.<locals>.replacement_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     72\u001b[0m     args \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(lst)\n\u001b[0;32m---> 73\u001b[0m \u001b[39mreturn\u001b[39;00m fdict[func_name](\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:331\u001b[0m, in \u001b[0;36mPyTorchClassifier.predict\u001b[0;34m(self, x, batch_size, training_mode, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m begin, end \u001b[39m=\u001b[39m (\n\u001b[1;32m    326\u001b[0m     m \u001b[39m*\u001b[39m batch_size,\n\u001b[1;32m    327\u001b[0m     \u001b[39mmin\u001b[39m((m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m batch_size, x_preprocessed\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]),\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    330\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 331\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(torch\u001b[39m.\u001b[39;49mfrom_numpy(x_preprocessed[begin:end])\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_device))\n\u001b[1;32m    332\u001b[0m output \u001b[39m=\u001b[39m model_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    333\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/art/estimators/classification/pytorch.py:1165\u001b[0m, in \u001b[0;36mPyTorchClassifier._make_model_wrapper.<locals>.ModelWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1162\u001b[0m         result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1164\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m-> 1165\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model(x)\n\u001b[1;32m   1166\u001b[0m     result\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1168\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m--> 154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_output = []\n",
    "iterations = 10\n",
    "# Voor elke hoeveelheid iteraties [1, 2, ..., 50], bereken de success rate/accuracy -> Dit zijn de X en Y coordinaten van de grafiek\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    # attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "    attack = BasicIterativeMethod(estimator=classifier, max_iter=i, batch_size=1, verbose = False)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(test_dataset):\n",
    "        # image is een afbeelding. wordt gegeven als tensor met shape [3, 32, 32]\n",
    "        # true_label is een simpele int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack.generate(image_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "        # Is de adversarial attack successvol of niet?\n",
    "        # Hoe definieer je een successvolle aanval?\n",
    "        # De definitie vanonder is gewoon een voorbeeld\n",
    "        if normal_prediction == true_label and normal_prediction != adversarial_prediction:\n",
    "            success_rates.append(True)\n",
    "        else:\n",
    "            success_rates.append(False)\n",
    "        # success_rates.append(success)\n",
    "        if j > 100:\n",
    "            break\n",
    "    ASR = success_rates.count(True) / len(success_rates)\n",
    "    accuracy_score = (1-ASR) \n",
    "    plot_output.append((i, accuracy_score))\n",
    "# Plot output should contain our X, y coordinates. List[(x, y), (x, y), ...]\n",
    "print(\"Average Success Rate: {:.2f}%\".format(ASR), plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [p[0] for p in plot_output]\n",
    "y = [p[1] for p in plot_output]\n",
    "# Plot the success rate over\n",
    "plt.plot(x, y, label = \"ASR\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('IFGSM Attack Success Rate')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, label = \"ASR\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('IFGSM Attack Success Rate')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.trainer import AdversarialTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_trainer = AdversarialTrainer(classifier= classifier, attacks = attack, ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (image, true_label) in enumerate(train_dataset):\n",
    "    # image is an image,\n",
    "    # true_label is a simple int\n",
    "    # Add batch dimension\n",
    "    image = torch.unsqueeze(image, dim=0) # shape == [1, 3, 32, 32]\n",
    "    image_numpy = image.numpy()\n",
    "    true_label_numpy = np.array([true_label])\n",
    "    # Generate adversarial sample\n",
    "    adv_trainer_fit = adv_trainer.fit(image_numpy, true_label_numpy, batch = 64, nb_epochs = 30) # shape == [1, 3, 32, 32]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your train and test dataset loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# define a function to train the model\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = (100 * correct / total)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "# define a function to test the model\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_acc = (100 * correct / total)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "# initialize the model, criterion, and optimizer\n",
    "model = resnet50(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 43)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the PyTorchClassifier\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    clip_values=(0.0, 1.0),\n",
    "    loss=criterion,\n",
    "    optimizer=optimizer,\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=43,\n",
    "    device_type=\"cpu\",\n",
    ")\n",
    "\n",
    "# Define an attack\n",
    "attack = BasicIterativeMethod(estimator=classifier, max_iter=10, batch_size=1)\n",
    "\n",
    "# adversarial training \n",
    "for j, (image, true_label) in enumerate(train_dataset):\n",
    "    # image is an image,\n",
    "    # true_label is a simple int\n",
    "    # Add batch dimension\n",
    "    image = torch.unsqueeze(image, dim=0) # shape == [1, 3, 32, 32]\n",
    "    image_numpy = image.numpy()\n",
    "    true_label_numpy = np.array([true_label])\n",
    "    # Generate adversarial sample\n",
    "    X_adv = attack.generate(image_numpy) # shape == [1, 3, 32, 32]\n",
    "    # Normal prediction\n",
    "    normal_prediction = torch.argmax(model(image)).item()\n",
    "    # Prediction of the adversarial sample\n",
    "    adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "    if normal_prediction != true_label:\n",
    "        print(f\"Original image was misclassified with true label: {true_label} and predicted label: {normal_prediction}\")\n",
    "    if adversarial_prediction != true_label:\n",
    "        print(f\"Adversarial image was misclassified with true label: {true_label} and predicted label: {adversarial_prediction}\")\n",
    "    if j > 100: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_adv = np.concatenate((image_numpy, X_adv))\n",
    "x_adv = [image, X_adv]\n",
    "# y_adv = np.concatenate((true_label_numpy, true_label_numpy))\n",
    "y_adv = np.array([true_label, true_label])\n",
    "y_adv = torch.from_numpy(y_adv)\n",
    "# y_adv = torch.from_numpy(y_adv)\n",
    "classifier.fit(x_adv, y_adv, nb_epochs=1, batch_size=1)\n",
    "\n",
    "torch.save(model, \"resnet50_adv_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial training iterations = 10\n",
    "# For each number of iterations [1, 2, ..., 10], calculate the success rate/accuracy -> these are the X and Y coordinates of the graph\n",
    "attack = BasicIterativeMethod(estimator=classifier, max_iter=10, batch_size=1)\n",
    "for j, (image, true_label) in enumerate(train_dataset):\n",
    "        # image is an image, given as a tensor with shape [3, 32, 32]\n",
    "        # true_label is a simple int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack.generate(image_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        # normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        # adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "\n",
    "        x_adv = np.concatenate((image, X_adv))\n",
    "        y_adv = np.concatenate((true_label_numpy, true_label_numpy))\n",
    "        y_adv = torch.from_numpy(y_adv)\n",
    "        y_adv = y_adv.cpu()\n",
    "        y_adv = y_adv.numpy()\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        # normal_prediction = torch.argmax(model(image)).item()\n",
    "\n",
    "\n",
    "        classifier.fit(x_adv, y_adv, nb_epochs=1, batch_size=1)\n",
    "        if j >= 10:\n",
    "            break\n",
    "torch.save(model, \"resnet50_adv_trained.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We wrap our model in a PyTorchClassifier, which ART knows how to use\n",
    "classifier = PyTorchClassifier(\n",
    "    # model=WrappedModel(),\n",
    "    model=model,\n",
    "    clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
    "    loss=criterion,  # defined above\n",
    "    optimizer=optimizer,  # defined above\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=43,\n",
    "    device_type=\"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(\"path/to/saved/model.pt\")\n",
    "\n",
    "# Evaluate the model on a dataset\n",
    "outputs = model(x_test)\n",
    "predictions = torch.argmax(outputs, dim=1)\n",
    "accuracy = torch.mean((predictions == y_test).float())\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial training \n",
    "\n",
    "plot_output = []\n",
    "iterations = 10\n",
    "\n",
    "# For each number of iterations [1, 2, ..., 10], calculate the success rate/accuracy -> these are the X and Y coordinates of the graph\n",
    "for i in range(iterations):\n",
    "    # Define an attack that only runs until this max iteration\n",
    "    attack = BasicIterativeMethod(estimator=classifier, max_iter=i, batch_size=1)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(train_dataset):\n",
    "        # image is an image, given as a tensor with shape [3, 32, 32]\n",
    "        # true_label is a simple int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack.generate(image_numpy)  # shape == [1, 3, 32, 32]\n",
    "        x_adv = np.concatenate((image_numpy, X_adv))\n",
    "        y_adv = np.concatenate((true_label_numpy, true_label_numpy))\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        # normal_prediction = torch.argmax(model(image)).item()\n",
    "\n",
    "        classifier.fit(x_adv, y_adv, nb_epochs=1, batch_size=1)\n",
    "        if j >= 10:\n",
    "            break\n",
    "classifier.save(\"resnet50_trained.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [p[0] for p in plot_output]\n",
    "y = [p[1] for p in plot_output]\n",
    "# Plot the success rate over\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title(' BIM Attack Success Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output = []\n",
    "iterations = 10\n",
    "# Voor elke hoeveelheid iteraties [1, 2, ..., 50], bereken de success rate/accuracy -> Dit zijn de X en Y coordinaten van de grafiek\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    # attack = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1)\n",
    "    attack = BasicIterativeMethod(estimator=classifier, max_iter=i, batch_size=1)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(train_dataset):\n",
    "        # image is een afbeelding. wordt gegeven als tensor met shape [3, 32, 32]\n",
    "        # true_label is een simpele int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack.generate(image_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "        # Is de adversarial attack successvol of niet?\n",
    "        # Hoe definieer je een successvolle aanval?\n",
    "        # De definitie vanonder is gewoon een voorbeeld\n",
    "        if normal_prediction == true_label and normal_prediction != adversarial_prediction:\n",
    "            success_rates.append(True)\n",
    "        else:\n",
    "            success_rates.append(False)\n",
    "        # success_rates.append(success)\n",
    "        if j >= 10:\n",
    "            break\n",
    "    accuracy_score = success_rates.count(True) / len(success_rates)\n",
    "    plot_output.append((i, accuracy_score))\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [p[0] for p in plot_output]\n",
    "# y = [p[1] for p in plot_output]\n",
    "# Plot the success rate over\n",
    "plt.plot(x_coords, y_coords)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.title('CW Attack Success Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(train_dataset):\n",
    "        # image is een afbeelding. wordt gegeven als tensor met shape [3, 32, 32]\n",
    "        # true_label is een simpele int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack_cw.generate(image_numpy, true_label_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "        # Is de adversarial attack successvol of niet?\n",
    "        # Hoe definieer je een successvolle aanval?\n",
    "        # De definitie vanonder is gewoon een voorbeeld\n",
    "        if normal_prediction == true_label and normal_prediction != adversarial_prediction:\n",
    "            correct += 0\n",
    "        # success_rates.append(success)\n",
    "\n",
    "        if j >= 10: \n",
    "            break\n",
    "    accuracy_score = correct / float(len(train_loader))\n",
    "    plot_output.append((i, accuracy_score))\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "        correct = 0\n",
    "        plot_output = []\n",
    "        attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "\n",
    "        for j, (image, true_label) in enumerate(test_dataset):\n",
    "                if j <= 5:\n",
    "                        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "                        image_numpy = image.numpy()\n",
    "                        true_label_numpy = np.array([true_label])\n",
    "                        X_adv = attack_cw.generate(image_numpy, true_label_numpy)  # shape == [1, 3, 32, 32]\n",
    "                        normal_prediction = torch.argmax(model(image)).item()\n",
    "                        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "                        if normal_prediction == true_label and adversarial_prediction != true_label:\n",
    "                                correct += 1\n",
    "        \n",
    "        accuracy_score = correct / float(len(test_loader))\n",
    "        plot_output.append((i, accuracy_score))\n",
    "\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output, accuracy_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output = []\n",
    "iterations = 3\n",
    "correct = 0\n",
    "# Voor elke hoeveelheid iteraties [1, 2, ..., 50], bereken de success rate/accuracy -> Dit zijn de X en Y coordinaten van de grafiek\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(train_dataset):\n",
    "        # image is een afbeelding. wordt gegeven als tensor met shape [3, 32, 32]\n",
    "        # true_label is een simpele int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy = image.numpy()\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack_cw.generate(image_numpy, true_label_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "        # Is de adversarial attack successvol of niet?\n",
    "        # Hoe definieer je een successvolle aanval?\n",
    "        # De definitie vanonder is gewoon een voorbeeld\n",
    "        if normal_prediction == true_label and normal_prediction != adversarial_prediction:\n",
    "            correct += 0\n",
    "        # success_rates.append(success)\n",
    "\n",
    "        if j >= 10: \n",
    "            break\n",
    "    accuracy_score = correct / float(len(train_loader))\n",
    "    plot_output.append((i, accuracy_score))\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    for j, (image, true_label) in enumerate(test_dataset):\n",
    "            success_rates = []\n",
    "        \n",
    "            attack_cw = CarliniL0Method(classifier=classifier, max_iter=5, batch_size=1, learning_rate=0.01)\n",
    "            image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "            # Omvormen naar numpy array\n",
    "            image_numpy = image.numpy()\n",
    "            true_label_numpy = np.array([true_label])\n",
    "            normal_prediction = torch.argmax(model(image)).item()\n",
    "            # Genereer adversarial sample\n",
    "            X_adv = attack_cw.generate(image_numpy, true_label_numpy)   \n",
    "            adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "            success = adversarial_prediction != true_label_numpy\n",
    "            success_rates.append(success)\n",
    "            if j >= 5:\n",
    "                break\n",
    "    accuracy_score = ((1- success) / len(true_label_numpy))\n",
    "    plot_output.append((i, accuracy_score))\n",
    "\n",
    "    print(X_adv, plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import compute_accuracy, compute_success_array, compute_success   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/utils.html#art.utils.compute_accuracy\n",
    "\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(train_dataset):\n",
    "                if j <= 5:\n",
    "                    image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "                    # Omvormen naar numpy array\n",
    "                    image_numpy = image.numpy()\n",
    "                    true_label_numpy = np.array([true_label])\n",
    "                    # Genereer adversarial sample\n",
    "                    X_adv = attack_cw.generate(image_numpy, true_label_numpy)  # shape == [1, 3, 32, 32]\n",
    "                    normal_prediction = torch.argmax(model(image)).item()\n",
    "                    # Prediction v/d adversarial sample\n",
    "                    adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "\n",
    "                    # success = normal_prediction == true_label and normal_prediction != adversarial_prediction\n",
    "                    \n",
    "\n",
    "                    # success_rates.append(success)\n",
    "    # accuracy_score = success_rates.count(True) / len(success_rates)\n",
    "    accuracy_score = compute_accuracy(preds = adversarial_prediction,  labels = normal_prediction, abstain = True)\n",
    "    plot_output.append((i, accuracy_score))\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "        success_rates = []\n",
    "        plot_output = []\n",
    "        for j, (image, true_label) in enumerate(train_dataset):\n",
    "                if j <= 5:\n",
    "                        attack_cw = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "                        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "                        # Omvormen naar numpy array\n",
    "                        image_numpy = image.numpy()\n",
    "                        true_label_numpy = np.array([true_label])\n",
    "                        # Genereer adversarial sample\n",
    "                        X_adv = attack_cw.generate(image_numpy, true_label_numpy)  # shape == [1, 3, 32, 32]\n",
    "                        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "                        normal_prediction = model(image)\n",
    "                        # Prediction van de adversarial sample\n",
    "                        adversarial_prediction = torch.argmax(model(torch.from_numpy(X_adv))).item()\n",
    "                        # De definitie vanonder is gewoon een voorbeeld\n",
    "                        accuracy_score = compute_success(classifier = classifier, x_clean = image_numpy, labels = true_label_numpy, x_adv = X_adv, targeted = False, batch_size= 1 )\n",
    "\n",
    "        plot_output.append((i, accuracy_score))\n",
    "\n",
    "# Plot output zou onze X, y coordinaten moeten bevatten. List[(x, y), (x, y), ...]\n",
    "print(plot_output, accuracy_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the CW attack on the model\n",
    "plot_output = cw_attack(iterations = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the success rate over iterations\n",
    "x = [p[0] for p in plot_output]\n",
    "y = [p[1] for p in plot_output]\n",
    "# Plot the success rate over\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy score')\n",
    "plt.title('CW Attack Success Rate')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFGSM attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFGSM adversarial training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = (sum(p.numel() for p in model.parameters()))\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "print(summary(model, (3, 32, 32))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "print(summary(model, (3, 32, 32))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sn \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "        output = model(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "# constant for classes\n",
    "classes = ( '0:Speed limit (20km/h)',\n",
    "            '1:Speed limit (30km/h)', \n",
    "            '2:Speed limit (50km/h)', \n",
    "            '3:Speed limit (60km/h)', \n",
    "            '4:Speed limit (70km/h)', \n",
    "            '5:Speed limit (80km/h)', \n",
    "            '6:End of speed limit (80km/h)', \n",
    "            '7:Speed limit (100km/h)', \n",
    "            '8:Speed limit (120km/h)', \n",
    "            '9:No passing', \n",
    "            '10:No passing veh over 3.5 tons', \n",
    "            '11:Right-of-way at intersection', \n",
    "            '12:Priority road', \n",
    "            '13:Yield', \n",
    "            '14:Stop', \n",
    "            '15:No vehicles', \n",
    "            '16:Veh > 3.5 tons prohibited', \n",
    "            '17:No entry', \n",
    "            '18:General caution', \n",
    "            '19:Dangerous curve left', \n",
    "            '20:Dangerous curve right', \n",
    "            '21:Double curve', \n",
    "            '22:Bumpy road', \n",
    "            '23:Slippery road', \n",
    "            '24:Road narrows on the right', \n",
    "            '25:Road work', \n",
    "            '26:Traffic signals', \n",
    "            '27:Pedestrians', \n",
    "            '28:Children crossing', \n",
    "            '29:Bicycles crossing', \n",
    "            '30:Beware of ice/snow',\n",
    "            '31:Wild animals crossing', \n",
    "            '32:End speed + passing limits', \n",
    "            '33:Turn right ahead', \n",
    "            '34:Turn left ahead', \n",
    "            '35:Ahead only', \n",
    "            '36:Go straight or right', \n",
    "            '37:Go straight or left', \n",
    "            '38:Keep right', \n",
    "            '39:Keep left', \n",
    "            '40:Roundabout mandatory', \n",
    "            '41:End of no passing', \n",
    "            '42:End no passing veh > 3.5 tons')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "#                      columns = [i for i in classes])\n",
    "# plt.figure(figsize = (40,15))\n",
    "# sn.heatmap(df_cm, annot=True)\n",
    "# plt.savefig('output.png')\n",
    "\n",
    "\n",
    "# print(cf_matrix)\n",
    "# cf_report = classification_report(y_true, y_pred)\n",
    "# print(cf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_test, y_pred):\n",
    "    '''Source: https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format'''\n",
    "    from sklearn import metrics\n",
    "    report = metrics.classification_report(y_test, y_pred, output_dict=True)\n",
    "    df_classification_report = pd.DataFrame(report).transpose()\n",
    "    df_classification_report = df_classification_report.sort_values(by=['f1-score'], ascending=False)\n",
    "    return df_classification_report\n",
    "\n",
    "\n",
    "get_classification_report(y_test = y_true, y_pred = y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, recall_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_true, y_pred)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_true, y_pred, average='micro')))\n",
    "print('Micro Sensitivity: {:.2f}\\n'.format(recall_score(y_true, y_pred, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_true, y_pred, average='macro')))\n",
    "print('Macro Sensitivity: {:.2f}\\n'.format(recall_score(y_true, y_pred, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_true, y_pred, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_true, y_pred, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_true, y_pred, average='weighted')))\n",
    "print('Weighted Sensitivity: {:.2f}\\n'.format(recall_score(y_true, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import seaborn as sn \n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (40,15))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('Resnet50.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(cr, title='Classification report ', with_avg_total=False, cmap=plt.cm.Blues):\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "\n",
    "    if with_avg_total:\n",
    "        aveTotal = lines[len(lines) - 1].split()\n",
    "        classes.append('avg/total')\n",
    "        vAveTotal = [float(x) for x in t[1:len(aveTotal) - 1]]\n",
    "        plotMat.append(vAveTotal)\n",
    "\n",
    "\n",
    "    plt.imshow(plotMat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    x_tick_marks = np.arange(3)\n",
    "    y_tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(x_tick_marks, ['precision', 'recall', 'f1-score'], rotation=45)\n",
    "    plt.yticks(y_tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Classes')\n",
    "    plt.xlabel('Measures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_true, y_pred) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM Attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = test_dataset[0]\n",
    "X.shape, X.min(), X.max()\n",
    "\n",
    "# class NormalizeInverse(torchvision.transforms.Normalize):\n",
    "#     \"\"\"\n",
    "#     Undoes the normalization and returns the reconstructed images in the input domain.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, mean, std):\n",
    "#         mean = torch.as_tensor(mean)\n",
    "#         std = torch.as_tensor(std)\n",
    "#         std_inv = 1 / (std + 1e-7)\n",
    "#         mean_inv = -mean * std_inv\n",
    "#         super().__init__(mean=mean_inv, std=std_inv)\n",
    "\n",
    "#     def __call__(self, tensor):\n",
    "#         return super().__call__(tensor.clone())\n",
    "\n",
    "\n",
    "# inverse_normalize = NormalizeInverse(mean=(0.3403, 0.3121, 0.3214), std=(0.2724, 0.2608, 0.2669))  # Same mean and std values as defined in the original transforms.\n",
    "# #torchvision.transforms.ToPILImage()((X))\n",
    "\n",
    "class WrappedModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.trained_model = model  # \"model\" is our trained model\n",
    "    self.trained_model.to(\"cpu\")\n",
    "    self.normalize = torchvision.transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669))\n",
    "\n",
    "  def forward(self, x: torch.Tensor):\n",
    "    # x comes in as a Tensor of shape [3, 32, 32]\n",
    "    # Manually add a batch dimension\n",
    "    x = torch.unsqueeze(x, dim=0)\n",
    "    # Shape of x is now [1, 3, 32, 32]\n",
    "    # Normalize the input\n",
    "    x_normalized = self.normalize(x)\n",
    "    # Return only the logits\n",
    "    logits = self.trained_model(x_normalized)\n",
    "    return logits\n",
    "\n",
    "# We again wrap our (already wrapped) model in a PyTorchClassifier, which ART knows how to use\n",
    "classifier = PyTorchClassifier(\n",
    "      model=WrappedModel(),\n",
    "      clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
    "      loss=criterion,  # defined above\n",
    "      optimizer=optimizer,  # defined above\n",
    "      input_shape=(3, 32, 32),\n",
    "      nb_classes=43,\n",
    "      device_type=\"cpu\"\n",
    ")\n",
    "\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "# benign_x = inverse_normalize(X)\n",
    "benign_x = X.cpu().numpy()  # convert torch to numpy\n",
    "type(benign_x), benign_x.min(), benign_x.max()\n",
    "x_adversarial = attack.generate(benign_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "#path = \"/volumes1/thesis/data/gtsrb/Adv_samples_resnet50_fgsm\"\n",
    "#os.mkdir(path)\n",
    "test_model = WrappedModel()\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "x_adv = []\n",
    "for i, (x, y_true) in enumerate(test_dataset):\n",
    "    benign_x = x.cpu().numpy()\n",
    "    y_predicted = torch.argmax(test_model(torch.Tensor(benign_x)))\n",
    "    if y_predicted == y_true:\n",
    "        x_adversarial = attack.generate(benign_x)\n",
    "        y_adversarial = torch.argmax(test_model(torch.Tensor(x_adversarial)))\n",
    "        if y_adversarial != y_predicted:\n",
    "            x_adv.append(x_adversarial)\n",
    "    if i > 100:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_adv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.evasion import CarliniL0Method, CarliniL2Method, CarliniLInfMethod\n",
    "from torchvision.datasets import GTSRB\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "class WrappedModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.trained_model = model  # \"model\" is our trained model\n",
    "        self.trained_model.to(\"cpu\")\n",
    "        self.normalize = torchvision.transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669))\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x comes in as a Tensor of shape [1, 3, 32, 32] (same as the input size passed to attack.generate())\n",
    "        # Normalize the input\n",
    "        x_normalized = self.normalize(x)\n",
    "        # Return only the logits\n",
    "        logits = self.trained_model(x_normalized)\n",
    "        return logits\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "wrapped_model = WrappedModel(model)\n",
    "normalize = torchvision.transforms.Normalize((0.3403, 0.3121, 0.3214), (0.2724, 0.2608, 0.2669))\n",
    "# We again wrap our (already wrapped) model in a PyTorchClassifier, which ART knows how to use\n",
    "classifier = PyTorchClassifier(\n",
    "    model=wrapped_model,\n",
    "    # model=model,\n",
    "    clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
    "    loss=criterion,  # defined above\n",
    "    optimizer=optimizer,  # defined above\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=43,\n",
    "    device_type=\"cpu\",\n",
    "    channels_first=True,\n",
    ")\n",
    "transforms = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.Resize((32, 32)), torchvision.transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "test_dataset = GTSRB(root=\"data\", split=\"test\", transform=transforms, download=True)\n",
    "x, y_true = test_dataset[0]\n",
    "# cw0_attack = CarliniL0Method(classifier, learning_rate=0.0001)\n",
    "# l2_attack = CarliniL2Method(classifier, learning_rate=0.0001)\n",
    "# linf_attack = CarliniLInfMethod(classifier, learning_rate=0.0001)\n",
    "# art_input = np.expand_dims(x.numpy(), axis=0)\n",
    "# adv_samples = cw0_attack.generate(art_input)\n",
    "# adv_samples = l2_attack.generate(art_input)\n",
    "# adv_samples = linf_attack.generate(art_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model = wrapped_model.to('cpu')\n",
    "tested_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "path = \"/volumes1/thesis/data/gtsrb/Adv_samples_resnet50_fgsm\"\n",
    "os.mkdir(path)\n",
    "for i, (x, y_true) in enumerate(test_dataset):\n",
    "    attack = FastGradientMethod(estimator=classifier, eps=0.1)\n",
    "    benign_x = x.cpu().numpy()\n",
    "    test_model = WrappedModel()\n",
    "    y_predicted = torch.argmax(test_model(torch.Tensor(benign_x)))\n",
    "    #type(benign_x), benign_x.min(), benign_x.max()\n",
    "    if y_predicted == y_true:\n",
    "        x_adversarial = attack.generate(benign_x)\n",
    "        y_adversarial = torch.argmax(test_model(torch.Tensor(x_adversarial)))\n",
    "        if y_adversarial != y_predicted:\n",
    "            np.save(f\"{path}/adv_samples_resnet50_fgsm_sample_{i}\", x_adversarial)\n",
    "    else:\n",
    "        print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = \"/volumes1/thesis/data/gtsrb/adv_samples_resnet50_l2\"\n",
    "#os.mkdir(path)\n",
    "l2_attack = CarliniL2Method(classifier, learning_rate=0.0001)\n",
    "for i, (x, y_true) in enumerate(test_dataset):\n",
    "    art_input = np.expand_dims(x.numpy(), axis=0)\n",
    "    y_predicted = torch.argmax(tested_model(torch.Tensor(art_input)))\n",
    "    if y_predicted == y_true:\n",
    "        adv_samples = l2_attack.generate(art_input) \n",
    "        y_adversarial = torch.argmax(tested_model(torch.Tensor(adv_samples)))\n",
    "        if y_adversarial != y_predicted:\n",
    "            np.save(f\"{path}/adv_samples_resnet50_l2_sample_{i}\", adv_samples)\n",
    "        else: \n",
    "            print('failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/volumes1/thesis/data/gtsrb/adv_samples_resnet50_linf\"\n",
    "os.mkdir(path)\n",
    "linf_attack = CarliniLInfMethod(classifier, learning_rate=0.0001)\n",
    "for i, (x, y_true) in enumerate(test_dataset):\n",
    "    art_input = np.expand_dims(x.numpy(), axis=0)\n",
    "    y_predicted = torch.argmax(tested_model(torch.Tensor(art_input)))\n",
    "    if y_predicted == y_true:\n",
    "        adv_samples = linf_attack.generate(art_input) \n",
    "        y_adversarial = torch.argmax(tested_model(torch.Tensor(adv_samples)))\n",
    "        if y_adversarial != y_predicted:\n",
    "            np.save(f\"{path}/adv_samples_resnet50_linf_sample_{i}\", adv_samples)\n",
    "        else: \n",
    "            print('failed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack trial\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import MIFGSM, CW, DeepFool, MultiAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, title):\n",
    "    npimg = img.numpy()\n",
    "    fig = plt.figure(figsize = (5, 15))\n",
    "    plt.imshow(np.transpose(npimg,(1,2,0)))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mifgsm_attack = MIFGSM(model, eps=8/255, alpha= 2/255, steps = 100)\n",
    "cw_attack = CW(model, steps = 100, c=1, kappa = 0, lr = 0.01)\n",
    "deepfool_attack = DeepFool(model, steps = 100, overshoot = 0.02)\n",
    "attacks = MultiAttack([mifgsm_attack, cw_attack, deepfool_attack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchattacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# atk = FGSM(model, eps=0.3)\n",
    "# atk1 = torchattacks.FGSM(model, eps=3)\n",
    "# #atk2 = torchattacks.PGD(model, eps=8/255, alpha=2/200, steps=40, random_start=True)\n",
    "# atk = torchattacks.MultiAttack([atk1])\n",
    "\n",
    "atks = [\n",
    "  CW(model, c=1, lr=0.01, steps=100, kappa=0),\n",
    "MIFGSM(model, eps=8/255, alpha=2/255, steps=100, decay=0.1),\n",
    "   DeepFool(model, steps=100),\n",
    "]\n",
    "\n",
    "#atk = torchattacks.MultiAttack(atks)\n",
    "\n",
    "\n",
    "\n",
    "for i in atks[:]:\n",
    "    for images, labels in test_loader:\n",
    "        images = i(images, labels).cpu()\n",
    "        outputs = model(images)\n",
    "    \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.cpu()).sum()\n",
    "    \n",
    "    print('Attack is ', str(i).split(\"(\")[0], ' and Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Attack Image & Predicted Label\")\n",
    "model.eval()\n",
    "for attack in attacks :\n",
    "    \n",
    "    print(\"-\"*70)\n",
    "    print(attack)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "\n",
    "        images = attack(images, labels)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += 1\n",
    "        correct += (pre == labels).sum()\n",
    "\n",
    "        imshow(torchvision.utils.make_grid(images.cpu().data, normalize=True), [test_dataset.classes[i] for i in pre])\n",
    "\n",
    "    print('Accuracy of test text: %f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def wrapper_method(func):\n",
    "    def wrapper_func(self, *args, **kwargs):\n",
    "        result = func(self, *args, **kwargs)\n",
    "        for atk in self.__dict__.get('_attacks').values():\n",
    "            eval(\"atk.\"+func.__name__+\"(*args, **kwargs)\")\n",
    "        return result\n",
    "    return wrapper_func\n",
    "\n",
    "\n",
    "class Attack(object):\n",
    "    r\"\"\"\n",
    "    Base class for all attacks.\n",
    "    .. note::\n",
    "        It automatically set device to the device where given model is.\n",
    "        It basically changes training mode to eval during attack process.\n",
    "        To change this, please see `set_model_training_mode`.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, model):\n",
    "        r\"\"\"\n",
    "        Initializes internal attack state.\n",
    "        Arguments:\n",
    "            name (str): name of attack.\n",
    "            model (torch.nn.Module): model to attack.\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack = name\n",
    "        self._attacks = OrderedDict()\n",
    "        \n",
    "        self.set_model(model)\n",
    "        self.device = next(model.parameters()).device\n",
    "\n",
    "        # Controls attack mode.\n",
    "        self.attack_mode = 'default'\n",
    "        self.supported_mode = ['default']\n",
    "        self.targeted = False\n",
    "        self._target_map_function = None\n",
    "\n",
    "        # Controls when normalization is used.\n",
    "        self.normalization_used = None\n",
    "        self._normalization_applied = None\n",
    "        self._set_auto_normalization_used(model)\n",
    "\n",
    "        # Controls model mode during attack.\n",
    "        self._model_training = False\n",
    "        self._batchnorm_training = False\n",
    "        self._dropout_training = False\n",
    "\n",
    "    def forward(self, inputs, labels=None, *args, **kwargs):\n",
    "        r\"\"\"\n",
    "        It defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @wrapper_method\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        self.model_name = str(model).split(\"(\")[0]\n",
    "\n",
    "    def get_logits(self, inputs, labels=None, *args, **kwargs):\n",
    "        if self._normalization_applied is False:\n",
    "            inputs = self.normalize(inputs)\n",
    "        logits = self.model(inputs)\n",
    "        return logits\n",
    "\n",
    "    @wrapper_method\n",
    "    def _set_normalization_applied(self, flag):\n",
    "        self._normalization_applied = flag\n",
    "    \n",
    "    @wrapper_method\n",
    "    def set_device(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    @wrapper_method\n",
    "    def _set_auto_normalization_used(self, model):\n",
    "        mean = getattr(model, 'mean', None)\n",
    "        std = getattr(model, 'std', None)\n",
    "        if (mean is not None) and (std is not None):\n",
    "            if isinstance(mean, torch.Tensor):\n",
    "                mean = mean.cpu().numpy()\n",
    "            if isinstance(std, torch.Tensor):\n",
    "                std = std.cpu().numpy()\n",
    "            if (mean != 0).all() or (std != 1).all():\n",
    "                self.set_normalization_used(mean, std)\n",
    "#                 logging.info(\"Normalization automatically loaded from `model.mean` and `model.std`.\")\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_normalization_used(self, mean, std):\n",
    "        self.normalization_used = {}\n",
    "        n_channels = len(mean)\n",
    "        mean = torch.tensor(mean).reshape(1, n_channels, 1, 1)\n",
    "        std = torch.tensor(std).reshape(1, n_channels, 1, 1)\n",
    "        self.normalization_used['mean'] = mean\n",
    "        self.normalization_used['std'] = std\n",
    "        self._normalization_applied = True\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        mean = self.normalization_used['mean'].to(inputs.device)\n",
    "        std = self.normalization_used['std'].to(inputs.device)\n",
    "        return (inputs - mean) / std\n",
    "\n",
    "    def inverse_normalize(self, inputs):\n",
    "        mean = self.normalization_used['mean'].to(inputs.device)\n",
    "        std = self.normalization_used['std'].to(inputs.device)\n",
    "        return inputs*std + mean\n",
    "\n",
    "    def get_mode(self):\n",
    "        r\"\"\"\n",
    "        Get attack mode.\n",
    "        \"\"\"\n",
    "        return self.attack_mode\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_mode_default(self):\n",
    "        r\"\"\"\n",
    "        Set attack mode as default mode.\n",
    "        \"\"\"\n",
    "        self.attack_mode = 'default'\n",
    "        self.targeted = False\n",
    "        print(\"Attack mode is changed to 'default.'\")\n",
    "\n",
    "    @wrapper_method\n",
    "    def _set_mode_targeted(self, mode):\n",
    "        if \"targeted\" not in self.supported_mode:\n",
    "            raise ValueError(\"Targeted mode is not supported.\")\n",
    "        self.targeted = True\n",
    "        self.attack_mode = mode\n",
    "        print(\"Attack mode is changed to '%s'.\"%mode)\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_mode_targeted_by_function(self, target_map_function):\n",
    "        r\"\"\"\n",
    "        Set attack mode as targeted.\n",
    "        Arguments:\n",
    "            target_map_function (function): Label mapping function.\n",
    "                e.g. lambda inputs, labels:(labels+1)%10.\n",
    "                None for using input labels as targeted labels. (Default)\n",
    "        \"\"\"\n",
    "        self._set_mode_targeted('targeted(custom)')\n",
    "        self._target_map_function = target_map_function\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_mode_targeted_random(self):\n",
    "        r\"\"\"\n",
    "        Set attack mode as targeted with random labels.\n",
    "        Arguments:\n",
    "            num_classses (str): number of classes.\n",
    "        \"\"\"\n",
    "        self._set_mode_targeted('targeted(random)')\n",
    "        self._target_map_function = self.get_random_target_label\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_mode_targeted_least_likely(self, kth_min=1):\n",
    "        r\"\"\"\n",
    "        Set attack mode as targeted with least likely labels.\n",
    "        Arguments:\n",
    "            kth_min (str): label with the k-th smallest probability used as target labels. (Default: 1)\n",
    "        \"\"\"\n",
    "        self._set_mode_targeted('targeted(least-likely)')\n",
    "        assert (kth_min > 0)\n",
    "        self._kth_min = kth_min\n",
    "        self._target_map_function = self.get_least_likely_label\n",
    "\n",
    "    @wrapper_method\n",
    "    def set_model_training_mode(self, model_training=False, batchnorm_training=False, dropout_training=False):\n",
    "        r\"\"\"\n",
    "        Set training mode during attack process.\n",
    "        Arguments:\n",
    "            model_training (bool): True for using training mode for the entire model during attack process.\n",
    "            batchnorm_training (bool): True for using training mode for batchnorms during attack process.\n",
    "            dropout_training (bool): True for using training mode for dropouts during attack process.\n",
    "        .. note::\n",
    "            For RNN-based models, we cannot calculate gradients with eval mode.\n",
    "            Thus, it should be changed to the training mode during the attack.\n",
    "        \"\"\"\n",
    "        self._model_training = model_training\n",
    "        self._batchnorm_training = batchnorm_training\n",
    "        self._dropout_training = dropout_training\n",
    "\n",
    "    @wrapper_method\n",
    "    def _change_model_mode(self, given_training):\n",
    "        if self._model_training:\n",
    "            self.model.train()\n",
    "            for _, m in self.model.named_modules():\n",
    "                if not self._batchnorm_training:\n",
    "                    if 'BatchNorm' in m.__class__.__name__:\n",
    "                        m = m.eval()\n",
    "                if not self._dropout_training:\n",
    "                    if 'Dropout' in m.__class__.__name__:\n",
    "                        m = m.eval()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "\n",
    "    @wrapper_method\n",
    "    def _recover_model_mode(self, given_training):\n",
    "        if given_training:\n",
    "            self.model.train()\n",
    "\n",
    "    def save(self, data_loader, save_path=None, verbose=True, return_verbose=False,\n",
    "             save_predictions=False, save_clean_inputs=False, save_type='float'):\n",
    "        r\"\"\"\n",
    "        Save adversarial inputs as torch.tensor from given torch.utils.data.DataLoader.\n",
    "        Arguments:\n",
    "            save_path (str): save_path.\n",
    "            data_loader (torch.utils.data.DataLoader): data loader.\n",
    "            verbose (bool): True for displaying detailed information. (Default: True)\n",
    "            return_verbose (bool): True for returning detailed information. (Default: False)\n",
    "            save_predictions (bool): True for saving predicted labels (Default: False)\n",
    "            save_clean_inputs (bool): True for saving clean inputs (Default: False)\n",
    "        \"\"\"\n",
    "        if save_path is not None:\n",
    "            adv_input_list = []\n",
    "            label_list = []\n",
    "            if save_predictions:\n",
    "                pred_list = []\n",
    "            if save_clean_inputs:\n",
    "                input_list = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        l2_distance = []\n",
    "\n",
    "        total_batch = len(data_loader)\n",
    "        given_training = self.model.training\n",
    "\n",
    "        for step, (inputs, labels) in enumerate(data_loader):\n",
    "            start = time.time()\n",
    "            adv_inputs = self.__call__(inputs, labels)\n",
    "            batch_size = len(inputs)\n",
    "\n",
    "            if verbose or return_verbose:\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.get_output_with_eval_nograd(adv_inputs)\n",
    "\n",
    "                    # Calculate robust accuracy\n",
    "                    _, pred = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    right_idx = (pred == labels.to(self.device))\n",
    "                    correct += right_idx.sum()\n",
    "                    rob_acc = 100 * float(correct) / total\n",
    "\n",
    "                    # Calculate l2 distance\n",
    "                    delta = (adv_inputs - inputs.to(self.device)).view(batch_size, -1)\n",
    "                    l2_distance.append(torch.norm(delta[~right_idx], p=2, dim=1))\n",
    "                    l2 = torch.cat(l2_distance).mean().item()\n",
    "\n",
    "                    # Calculate time computation\n",
    "                    progress = (step+1)/total_batch*100\n",
    "                    end = time.time()\n",
    "                    elapsed_time = end-start\n",
    "\n",
    "                    if verbose:\n",
    "                        self._save_print(progress, rob_acc, l2, elapsed_time, end='\\r')\n",
    "\n",
    "            if save_path is not None:\n",
    "                adv_input_list.append(adv_inputs.detach().cpu())\n",
    "                label_list.append(labels.detach().cpu())\n",
    "\n",
    "                adv_input_list_cat = torch.cat(adv_input_list, 0)\n",
    "                label_list_cat = torch.cat(label_list, 0)\n",
    "                \n",
    "                save_dict = {'adv_inputs':adv_input_list_cat, 'labels':label_list_cat}\n",
    "\n",
    "                if save_predictions:\n",
    "                    pred_list.append(pred.detach().cpu())\n",
    "                    pred_list_cat = torch.cat(pred_list, 0)\n",
    "                    save_dict['preds'] = pred_list_cat\n",
    "\n",
    "                if save_clean_inputs:\n",
    "                    input_list.append(inputs.detach().cpu())\n",
    "                    input_list_cat = torch.cat(input_list, 0)\n",
    "                    save_dict['clean_inputs'] = input_list_cat\n",
    "                    \n",
    "                if self.normalization_used is not None:\n",
    "                    save_dict['adv_inputs'] = self.inverse_normalize(save_dict['adv_inputs'])\n",
    "                    if save_clean_inputs:\n",
    "                        save_dict['clean_inputs'] = self.inverse_normalize(save_dict['clean_inputs'])\n",
    "\n",
    "                if save_type == 'int':\n",
    "                    save_dict['adv_inputs'] = self.to_type(save_dict['adv_inputs'], 'int')\n",
    "                    if save_clean_inputs:\n",
    "                        save_dict['clean_inputs'] = self.to_type(save_dict['clean_inputs'], 'int')\n",
    "\n",
    "                save_dict['save_type'] = save_type\n",
    "                torch.save(save_dict, save_path)\n",
    "\n",
    "        # To avoid erasing the printed information.\n",
    "        if verbose:\n",
    "            self._save_print(progress, rob_acc, l2, elapsed_time, end='\\n')\n",
    "\n",
    "        if given_training:\n",
    "            self.model.train()\n",
    "\n",
    "        if return_verbose:\n",
    "            return rob_acc, l2, elapsed_time\n",
    "\n",
    "    @staticmethod\n",
    "    def to_type(inputs, type):\n",
    "        r\"\"\"\n",
    "        Return inputs as int if float is given.\n",
    "        \"\"\"\n",
    "        if type == 'int':\n",
    "            if isinstance(inputs, torch.FloatTensor) or isinstance(inputs, torch.cuda.FloatTensor):\n",
    "                return (inputs*255).type(torch.uint8)\n",
    "        elif type == 'float':\n",
    "            if isinstance(inputs, torch.ByteTensor) or isinstance(inputs, torch.cuda.ByteTensor):\n",
    "                return inputs.float()/255\n",
    "        else:\n",
    "            raise ValueError(type + \" is not a valid type. [Options: float, int]\")\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_print(progress, rob_acc, l2, elapsed_time, end):\n",
    "        print('- Save progress: %2.2f %% / Robust accuracy: %2.2f %% / L2: %1.5f (%2.3f it/s) \\t' \\\n",
    "              % (progress, rob_acc, l2, elapsed_time), end=end)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(load_path, batch_size=128, shuffle=False, normalize=None,\n",
    "             load_predictions=False, load_clean_inputs=False):\n",
    "        save_dict = torch.load(load_path)\n",
    "        keys = ['adv_inputs', 'labels']\n",
    "\n",
    "        if load_predictions:\n",
    "            keys.append('preds')\n",
    "        if load_clean_inputs:\n",
    "            keys.append('clean_inputs')\n",
    "\n",
    "        if save_dict['save_type'] == 'int':\n",
    "            save_dict['adv_inputs'] = save_dict['adv_inputs'].float()/255\n",
    "            if load_clean_inputs:\n",
    "                save_dict['clean_inputs'] = save_dict['clean_inputs'].float()/255\n",
    "                \n",
    "        if normalize is not None:\n",
    "            n_channels = len(normalize['mean'])\n",
    "            mean = torch.tensor(normalize['mean']).reshape(1, n_channels, 1, 1)\n",
    "            std = torch.tensor(normalize['std']).reshape(1, n_channels, 1, 1)\n",
    "            save_dict['adv_inputs'] = (save_dict['adv_inputs'] - mean) / std\n",
    "            if load_clean_inputs:\n",
    "                save_dict['clean_inputs'] = (save_dict['clean_inputs'] - mean) / std\n",
    "\n",
    "        adv_data = TensorDataset(*[save_dict[key] for key in keys])\n",
    "        adv_loader = DataLoader(adv_data, batch_size=batch_size, shuffle=shuffle)\n",
    "        print(\"Data is loaded in the following order: [%s]\"%(\", \".join(keys)))\n",
    "        return adv_loader\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_output_with_eval_nograd(self, inputs):\n",
    "        given_training = self.model.training\n",
    "        if given_training:\n",
    "            self.model.eval()\n",
    "        outputs = self.get_logits(inputs)\n",
    "        if given_training:\n",
    "            self.model.train()\n",
    "        return outputs\n",
    "\n",
    "    def get_target_label(self, inputs, labels=None):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return input labels.\n",
    "        \"\"\"\n",
    "        if self._target_map_function is None:\n",
    "            raise ValueError('target_map_function is not initialized by set_mode_targeted.')\n",
    "        target_labels = self._target_map_function(inputs, labels)\n",
    "        return target_labels\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_least_likely_label(self, inputs, labels=None):\n",
    "        outputs = self.get_output_with_eval_nograd(inputs)\n",
    "        if labels is None:\n",
    "            _, labels = torch.max(outputs, dim=1)\n",
    "        n_classses = outputs.shape[-1]\n",
    "\n",
    "        target_labels = torch.zeros_like(labels)\n",
    "        for counter in range(labels.shape[0]):\n",
    "            l = list(range(n_classses))\n",
    "            l.remove(labels[counter])\n",
    "            _, t = torch.kthvalue(outputs[counter][l], self._kth_min)\n",
    "            target_labels[counter] = l[t]\n",
    "\n",
    "        return target_labels.long().to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_random_target_label(self, inputs, labels=None):\n",
    "        outputs = self.get_output_with_eval_nograd(inputs)\n",
    "        if labels is None:\n",
    "            _, labels = torch.max(outputs, dim=1)\n",
    "        n_classses = outputs.shape[-1]\n",
    "\n",
    "        target_labels = torch.zeros_like(labels)\n",
    "        for counter in range(labels.shape[0]):\n",
    "            l = list(range(n_classses))\n",
    "            l.remove(labels[counter])\n",
    "            t = (len(l)*torch.rand([1])).long().to(self.device)\n",
    "            target_labels[counter] = l[t]\n",
    "\n",
    "        return target_labels.long().to(self.device)\n",
    "\n",
    "    def __call__(self, inputs, labels=None, *args, **kwargs):\n",
    "        given_training = self.model.training\n",
    "        self._change_model_mode(given_training)\n",
    "\n",
    "        if self._normalization_applied is True:\n",
    "            inputs = self.inverse_normalize(inputs)\n",
    "            self._set_normalization_applied(False)\n",
    "\n",
    "            adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
    "            adv_inputs = self.normalize(adv_inputs)\n",
    "            self._set_normalization_applied(True)\n",
    "        else:\n",
    "            adv_inputs = self.forward(inputs, labels, *args, **kwargs)\n",
    "\n",
    "        self._recover_model_mode(given_training)\n",
    "\n",
    "        return adv_inputs\n",
    "\n",
    "    def __repr__(self):\n",
    "        info = self.__dict__.copy()\n",
    "\n",
    "        del_keys = ['model', 'attack', 'supported_mode']\n",
    "\n",
    "        for key in info.keys():\n",
    "            if key[0] == \"_\":\n",
    "                del_keys.append(key)\n",
    "\n",
    "        for key in del_keys:\n",
    "            del info[key]\n",
    "\n",
    "        info['attack_mode'] = self.attack_mode\n",
    "        info['normalization_used'] = True if self.normalization_used is not None else False\n",
    "\n",
    "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        object.__setattr__(self, name, value)\n",
    "        \n",
    "        attacks = self.__dict__.get('_attacks')\n",
    "\n",
    "        # Get all items in iterable items.\n",
    "        def get_all_values(items, stack=[]):\n",
    "            if (items not in stack):\n",
    "                stack.append(items)\n",
    "                if isinstance(items, list) or isinstance(items, dict):\n",
    "                    if isinstance(items, dict):\n",
    "                        items = (list(items.keys())+list(items.values()))\n",
    "                    for item in items:\n",
    "                        yield from get_all_values(item, stack)\n",
    "                else:\n",
    "                    if isinstance(items, Attack):\n",
    "                        yield items\n",
    "            else:\n",
    "                if isinstance(items, Attack):\n",
    "                    yield items\n",
    "                \n",
    "        for num, value in enumerate(get_all_values(value)):\n",
    "            attacks[name+\".\"+str(num)] = value\n",
    "            for subname, subvalue in value.__dict__.get('_attacks').items():\n",
    "                attacks[name+\".\"+subname] = subvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttack(Attack):\n",
    "    r\"\"\"\n",
    "    MultiAttack is a class to attack a model with various attacks agains same images and labels.\n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        attacks (list): list of attacks.\n",
    "    Examples::\n",
    "        >>> atk1 = torchattacks.PGD(model, eps=8/255, alpha=2/255, iters=40, random_start=True)\n",
    "        >>> atk2 = torchattacks.PGD(model, eps=8/255, alpha=2/255, iters=40, random_start=True)\n",
    "        >>> atk = torchattacks.MultiAttack([atk1, atk2])\n",
    "        >>> adv_images = attack(images, labels)\n",
    "    \"\"\"\n",
    "    def __init__(self, attacks, verbose=False):\n",
    "        super().__init__(\"MultiAttack\", attacks[0].model)\n",
    "        self.attacks = attacks\n",
    "        self.verbose = verbose\n",
    "        self.supported_mode = ['default']\n",
    "\n",
    "        self.check_validity()\n",
    "\n",
    "        self._accumulate_multi_atk_records = False\n",
    "        self._multi_atk_records = [0.0]\n",
    "\n",
    "    def check_validity(self):\n",
    "        if len(self.attacks) < 2:\n",
    "            raise ValueError(\"More than two attacks should be given.\")\n",
    "\n",
    "        ids = [id(attack.model) for attack in self.attacks]\n",
    "        if len(set(ids)) != 1:\n",
    "            raise ValueError(\"At least one of attacks is referencing a different model.\")\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        fails = torch.arange(batch_size).to(self.device)\n",
    "        final_images = images.clone().detach().to(self.device)\n",
    "        labels = labels.clone().detach().to(self.device)\n",
    "\n",
    "        multi_atk_records = [batch_size]\n",
    "\n",
    "        for _, attack in enumerate(self.attacks):\n",
    "            adv_images = attack(images[fails], labels[fails])\n",
    "\n",
    "            outputs = self.get_logits(adv_images)\n",
    "            _, pre = torch.max(outputs.data, 1)\n",
    "\n",
    "            corrects = (pre == labels[fails])\n",
    "            wrongs = ~corrects\n",
    "\n",
    "            succeeds = torch.masked_select(fails, wrongs)\n",
    "            succeeds_of_fails = torch.masked_select(torch.arange(fails.shape[0]).to(self.device), wrongs)\n",
    "\n",
    "            final_images[succeeds] = adv_images[succeeds_of_fails]\n",
    "\n",
    "            fails = torch.masked_select(fails, corrects)\n",
    "            multi_atk_records.append(len(fails))\n",
    "\n",
    "            if len(fails) == 0:\n",
    "                break\n",
    "\n",
    "        if self.verbose:\n",
    "            print(self._return_sr_record(multi_atk_records))\n",
    "\n",
    "        if self._accumulate_multi_atk_records:\n",
    "            self._update_multi_atk_records(multi_atk_records)\n",
    "\n",
    "        return final_images\n",
    "\n",
    "    def _clear_multi_atk_records(self):\n",
    "        self._multi_atk_records = [0.0]\n",
    "\n",
    "    def _covert_to_success_rates(self, multi_atk_records):\n",
    "        sr = [((1-multi_atk_records[i]/multi_atk_records[0])*100) for i in range(1, len(multi_atk_records))]\n",
    "        return sr\n",
    "\n",
    "    def _return_sr_record(self, multi_atk_records):\n",
    "        sr = self._covert_to_success_rates(multi_atk_records)\n",
    "        return \"Attack success rate: \"+\" | \".join([\"%2.2f %%\"%item for item in sr])\n",
    "\n",
    "    def _update_multi_atk_records(self, multi_atk_records):\n",
    "        for i, item in enumerate(multi_atk_records):\n",
    "            self._multi_atk_records[i] += item\n",
    "\n",
    "    def save(self, data_loader, save_path=None, verbose=True, return_verbose=False,\n",
    "             save_predictions=False, save_clean_images=False):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        self._clear_multi_atk_records()\n",
    "        prev_verbose = self.verbose\n",
    "        self.verbose = False\n",
    "        self._accumulate_multi_atk_records = True\n",
    "\n",
    "        for i, attack in enumerate(self.attacks):\n",
    "            self._multi_atk_records.append(0.0)\n",
    "\n",
    "        if return_verbose:\n",
    "            rob_acc, l2, elapsed_time = super().save(data_loader, save_path,\n",
    "                                                     verbose, return_verbose,\n",
    "                                                     save_predictions,\n",
    "                                                     save_clean_images)\n",
    "            sr = self._covert_to_success_rates(self._multi_atk_records)\n",
    "        elif verbose:\n",
    "            super().save(data_loader, save_path, verbose,\n",
    "                         return_verbose, save_predictions,\n",
    "                         save_clean_images)\n",
    "            sr = self._covert_to_success_rates(self._multi_atk_records)\n",
    "        else:\n",
    "            super().save(data_loader, save_path, False,\n",
    "                         False, save_predictions,\n",
    "                         save_clean_images)\n",
    "\n",
    "        self._clear_multi_atk_records()\n",
    "        self._accumulate_multi_atk_records = False\n",
    "        self.verbose = prev_verbose\n",
    "\n",
    "        if return_verbose:\n",
    "            return rob_acc, sr, l2, elapsed_time\n",
    "\n",
    "    def _save_print(self, progress, rob_acc, l2, elapsed_time, end):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        print(\"- Save progress: %2.2f %% / Robust accuracy: %2.2f %%\"%(progress, rob_acc)+\\\n",
    "              \" / \"+self._return_sr_record(self._multi_atk_records)+\\\n",
    "              ' / L2: %1.5f (%2.3f it/s) \\t'%(l2, elapsed_time), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mifgsm_attack = MIFGSM(model, eps=8/255, alpha= 2/255, steps = 100)\n",
    "cw_attack = CW(model, steps = 100, c=1, kappa = 0, lr = 0.01)\n",
    "deepfool_attack = DeepFool(model, steps = 100, overshoot = 0.02)\n",
    "attacks = MultiAttack([mifgsm_attack, cw_attack, deepfool_attack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_images = attacks(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiAttack([mifgsm_attack, cw_attack, deepfool_attack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
