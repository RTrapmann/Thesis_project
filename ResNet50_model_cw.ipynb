{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet50 model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "from PIL import Image\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from torchvision.models.resnet import resnet50\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import art\n",
    "from art import attacks \n",
    "from art.attacks.evasion import FastGradientMethod, DeepFool, CarliniL0Method, BasicIterativeMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from art.defences.trainer import AdversarialTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining hyperparameters  \n",
    "epochs = 30  #the nn will train 29 times \n",
    "learning_rate = 0.0001 #how much the weight will be updated each time \n",
    "batch_size = 64 \n",
    "classes = 43 \n",
    "img_size = 32\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.3403, 0.3121, 0.3214),\n",
    "                        (0.2724, 0.2608, 0.2669))\n",
    "])\n",
    "\n",
    "\n",
    "#set 1\n",
    "# transforms = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomRotation(degrees = (0,1))  \n",
    "\n",
    "# ])\n",
    "\n",
    "# #set 2\n",
    "# transforms = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.GaussianBlur(kernel_size = (5,9)),\n",
    "# ])\n",
    "\n",
    "# #set 3\n",
    "# transforms = transforms.Compose([\n",
    "#     transforms.ColorJitter(brightness = .5, hue = .3),\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.3403, 0.3121, 0.3214),\n",
    "#                          (0.2724, 0.2608, 0.2669)),\n",
    "#     transforms.GaussianBlur(kernel_size = (5,9)),\n",
    "#     transforms.RandomRotation(degrees = (0, 1))\n",
    "# ])\n",
    "\n",
    "\n",
    "# #set 4\n",
    "# transforms = transforms.Compose([\n",
    "#     transforms.Resize((32, 32)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.3403, 0.3121, 0.3214),\n",
    "#                          (0.2724, 0.2608, 0.2669)),\n",
    "#     transforms.ElasticTransform(alpha = 1.0)          # to re-create rainy weather conditions \n",
    "# ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.GTSRB(\n",
    "    root='./data', split = 'test', transform=transforms, download=True)\n",
    "\n",
    "# train loader \n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size\n",
    ", shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<torch.utils.data.dataloader.DataLoader object at 0x7fc6b76c7a60>, <torch.utils.data.dataloader.DataLoader object at 0x7fc5caff5040>)\n"
     ]
    }
   ],
   "source": [
    "def get_train_valid_loader(\n",
    "                           batch_size,\n",
    "                           augment,\n",
    "                           random_seed,\n",
    "                           valid_size=0.1,\n",
    "                           shuffle=True,\n",
    "                           num_workers=2):\n",
    "\n",
    "    error_msg = \"[!] valid_size should be in the range [0, 1].\"\n",
    "    assert ((valid_size >= 0) and (valid_size <= 1)), error_msg\n",
    "\n",
    "\n",
    "    # load the dataset\n",
    "    global base_dataset\n",
    "    base_dataset = torchvision.datasets.GTSRB(\n",
    "    root='./data', split = 'train', transform=transforms, download=True)\n",
    "\n",
    "    # TODO\n",
    "    split_datasets = torch.utils.data.random_split(base_dataset, [0.20,0.8])\n",
    "    global val_dataset, train_dataset\n",
    "    val_dataset = split_datasets[0]\n",
    "    train_dataset = split_datasets[1]\n",
    "    \n",
    "\n",
    "    global num_train \n",
    "    num_train= len(train_dataset)\n",
    "    indices = list(range(num_train))\n",
    "    global split \n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "\n",
    "    #train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    #train_sampler = SubsetRandomSampler(train_idx)\n",
    "    #valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "    global train_loader \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        #sampler = train_sampler\n",
    "    )\n",
    "    global valid_loader \n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size,\n",
    "        num_workers=num_workers, \n",
    "        #sampler = valid_sampler\n",
    "    )\n",
    "\n",
    "    return train_loader, valid_loader\n",
    "\n",
    "print(get_train_valid_loader(batch_size = 64, augment = True, random_seed = 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_loader, device):\n",
    "    '''\n",
    "    Function for computing the accuracy of the predictions over the entire data_loader\n",
    "    '''\n",
    "    \n",
    "    correct_pred = 0 \n",
    "    n = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for X, y_true in data_loader:\n",
    "\n",
    "            X = X.to(device)\n",
    "            y_true = y_true.to(device)\n",
    "\n",
    "            y_prob = model(X)\n",
    "            _, predicted_labels = torch.max(y_prob, 1)\n",
    "\n",
    "            n += y_true.size(0)\n",
    "            correct_pred += (predicted_labels == y_true).sum()\n",
    "\n",
    "    return correct_pred.float() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, valid_losses):\n",
    "    '''\n",
    "    Function for plotting training and validation losses\n",
    "    '''\n",
    "\n",
    "    train_losses = np.array(train_losses) \n",
    "    valid_losses = np.array(valid_losses)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
    "\n",
    "    ax.plot(train_losses, color='blue', label='Training loss') \n",
    "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
    "    ax.set(title=\"Loss over epochs\", \n",
    "            xlabel='Epoch',\n",
    "            ylabel='Loss') \n",
    "    ax.legend()\n",
    "    fig.show()\n",
    "    \n",
    "    # change the plot style to default\n",
    "    plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    '''\n",
    "    Function for the training step of the training loop\n",
    "    '''\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        y_hat= model(X) \n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return model, optimizer, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation function, without a learning step (backward pass)\n",
    "\n",
    "def validate(valid_loader, model, criterion, device):\n",
    "    '''\n",
    "    Function for the validation step of the training loop\n",
    "    '''\n",
    "   \n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    \n",
    "    for X, y_true in valid_loader:\n",
    "    \n",
    "        X = X.to(device)\n",
    "        y_true = y_true.to(device)\n",
    "\n",
    "        # Forward pass and record loss\n",
    "        y_hat= model(X)    # predicted\n",
    "        loss = criterion(y_hat, y_true) \n",
    "        running_loss += loss.item() * X.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
    "        \n",
    "    return model, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function\n",
    "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every=1):\n",
    "    '''\n",
    "    Function defining the entire training loop\n",
    "    '''\n",
    "    \n",
    "    # set objects for storing metrics\n",
    "    best_loss = 1e10\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    # Train model\n",
    "    for epoch in range(0, epochs):\n",
    "\n",
    "        # training\n",
    "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # validation\n",
    "        with torch.no_grad():\n",
    "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "        if epoch % print_every == (print_every - 1):\n",
    "            \n",
    "            train_acc = get_accuracy(model, train_loader, device=device)\n",
    "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
    "                \n",
    "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
    "                  f'Epoch: {epoch}\\t'\n",
    "                  f'Train loss: {train_loss:.4f}\\t'\n",
    "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
    "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
    "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
    "\n",
    "    plot_losses(train_losses, valid_losses)\n",
    "    \n",
    "    return model, optimizer, (train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=False)\n",
    "model.fc = torch.nn.Linear(2048,43)\n",
    "model.conv1 = torch.nn.Conv2d(3,64,kernel_size=5,stride=1)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(random_seed)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, optimizer, _ \u001b[39m=\u001b[39m training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device)\n",
      "Cell \u001b[0;32mIn [12], line 15\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(model, criterion, optimizer, train_loader, valid_loader, epochs, device, print_every)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, epochs):\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m     \u001b[39m# training\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     model, optimizer, train_loss \u001b[39m=\u001b[39m train(train_loader, model, criterion, optimizer, device)\n\u001b[1;32m     16\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     18\u001b[0m     \u001b[39m# validation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [10], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m     running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     25\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m epoch_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_loader\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, optimizer, _ = training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"resnet18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=43, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"resnet18.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW attack "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap our model in a PyTorchClassifier, which ART knows how to use\n",
    "classifier = PyTorchClassifier(\n",
    "    # model=WrappedModel(),\n",
    "    model=model,\n",
    "    clip_values=(0.0, 1.0),  # The adversarial sample tensor values will be within these values\n",
    "    loss=criterion,  # defined above\n",
    "    optimizer=optimizer,  # defined above\n",
    "    input_shape=(3, 32, 32),\n",
    "    nb_classes=43,\n",
    "    device_type=\"cpu\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHANGE output name to set "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_output_base = []\n",
    "iterations = 10\n",
    "# Voor elke hoeveelheid iteraties [1, 2, ..., 50], bereken de success rate/accuracy -> Dit zijn de X en Y coordinaten van de grafiek\n",
    "for i in range(iterations):\n",
    "    # Define een attack die alleen runt tot deze max iterations!\n",
    "    # Batch size is 1 omdat we hieronder ook over de dataset itereren in batches van 1\n",
    "    attack = CarliniL0Method(classifier=classifier, max_iter=i, batch_size=1, learning_rate=0.01)\n",
    "    # attack = BasicIterativeMethod(estimator=classifier, max_iter=i, batch_size=1, verbose = False)\n",
    "    success_rates = []\n",
    "    for j, (image, true_label) in enumerate(test_dataset):\n",
    "        # image is een afbeelding. wordt gegeven als tensor met shape [3, 32, 32]\n",
    "        # true_label is een simpele int\n",
    "        # Add batch dimension\n",
    "        image = torch.unsqueeze(image, dim=0)  # shape == [1, 3, 32, 32]\n",
    "        # Omvormen naar numpy array\n",
    "        image_numpy =  np.float32(image)\n",
    "        true_label_numpy = np.array([true_label])\n",
    "        # Genereer adversarial sample\n",
    "        X_adv = attack.generate(image_numpy)  # shape == [1, 3, 32, 32]\n",
    "        # Model prediction op de gewone image. Deze kan ook fout zijn ten opzichte van het echte label!\n",
    "        normal_prediction = torch.argmax(model(image)).item()\n",
    "        # Prediction van de adversarial sample\n",
    "        adv_input = (torch.from_numpy(X_adv)).to(torch.float32)\n",
    "        adversarial_prediction = torch.argmax(adv_input).item()\n",
    "        # Is de adversarial attack successvol of niet?\n",
    "        # Hoe definieer je een successvolle aanval?\n",
    "        # De definitie vanonder is gewoon een voorbeeld\n",
    "        if normal_prediction == true_label and normal_prediction != adversarial_prediction:\n",
    "            success_rates.append(True)\n",
    "        else:\n",
    "            success_rates.append(False)\n",
    "        # success_rates.append(success)\n",
    "        if j > 50:\n",
    "            break\n",
    "    ASR = success_rates.count(True) / len(success_rates)\n",
    "    accuracy_score = (1-ASR) \n",
    "    plot_output_base.append((i, accuracy_score))\n",
    "# Plot output should contain our X, y coordinates. List[(x, y), (x, y), ...]\n",
    "print(\"Average Success Rate: {:.2f}%\".format(ASR), plot_output_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the success rate over\n",
    "x = [p[0] for p in plot_output_base]\n",
    "y = [p[1] for p in plot_output_base]\n",
    "# Plot the success rate over\n",
    "plt.plot(x, y, label = \"Accuracy\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('CW Attack Success Rate')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
